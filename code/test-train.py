import os, json
import pandas as pd
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    # DataCollatorWithPadding,
    # EvalPrediction,
    # HfArgumentParser,
    Trainer,
    TrainingArguments,
    set_seed,
)
from datasets import Dataset, DatasetDict, load_from_disk
import torch
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score




def main():
    config = AutoConfig.from_pretrained(
        "klue/roberta-large",
        num_labels=31)
    tokenizer = AutoTokenizer.from_pretrained("klue/roberta-large", use_fast=True)
    model = AutoModelForSequenceClassification.from_pretrained(
        "klue/roberta-large", 
        config=config, 
        )

    # print(
    #     config,
    #     tokenizer,
    #     model
    # )

    train_data_path = "../data/aihub_kr_hair/drop_dup/training"
    valid_data_path = "../data/aihub_kr_hair/drop_dup/validation"
    # print(os.listdir(train_data_path))
    train_dataset = load_from_disk(train_data_path)
    validation_dataset = load_from_disk(valid_data_path)

    print(train_dataset)
    print(validation_dataset)

    def label_to_num(label):
        labels = {
            'ê°€ë¥´ë§ˆ': 0,
            'ê¸°íƒ€ë‚¨ììŠ¤íƒ€ì¼': 1,
            'ê¸°íƒ€ë ˆì´ì–´ë“œ': 2,
            'ê¸°íƒ€ì—¬ììŠ¤íƒ€ì¼': 3,
            'ë‚¨ìì¼ë°˜ìˆ': 4,
            'ëŒ„ë””': 5,
            'ë£¨í”„': 6,
            'ë¦¬ì  íŠ¸': 7,
            'ë¦¬í”„': 8,
            'ë¯¸ìŠ¤í‹°': 9,
            'ë°”ë””': 10,
            'ë² ì´ë¹„': 11,
            'ë³´ë‹ˆ': 12,
            'ë³´ë¸Œ': 13,
            'ë¹Œë“œ': 14,
            'ì†Œí”„íŠ¸íˆ¬ë¸”ëŸ­ëŒ„ë””': 15,
            'ìˆë‹¨ë°œ': 16,
            'ì‰ë„ìš°': 17,
            'ì‰¼í‘œ': 18,
            'ìŠ¤í•€ìŠ¤ì™ˆë¡œ': 19,
            'ì‹œìŠ¤ë£¨ëŒ„ë””': 20,
            'ì• ì¦ˆ': 21,
            'ì—ì–´': 22,
            'ì—¬ìì¼ë°˜ìˆ': 23,
            'ì›ë­ìŠ¤': 24,
            'ì›ë¸”ëŸ­ëŒ„ë””': 25,
            'í…ŒìŠ¬': 26,
            'í¬ë§ˆë“œ': 27,
            'í”Œë¦¬ì¸ ': 28,
            'í—ˆì‰¬': 29,
            'íˆí”¼': 30
        }

        return labels[label]

    def pre_processing_dataset(example):
        input_str = f"""
        ë¨¸ë¦¬ ê¸¸ì´ëŠ” {example['basestyle-type']}, ê³±ìŠ¬ê¸°ëŠ” {example['curl']}, ì•ë¨¸ë¦¬ëŠ” {example['bang']}, ì˜†ë¨¸ë¦¬ëŠ” {example['side']}, ê°€ë¥´ë§ˆëŠ” {example['partition']}, ì„±ë³„ì€ {example['sex']}, ëª¨ë°œ êµµê¸°ëŠ” {example['hair-width']}
        """
        inputs = tokenizer(
            input_str,
            padding='max_length')
        inputs['labels'] = label_to_num(example['basestyle'])

        return inputs

    print("Start Preprocessing training dataset ...")
    train_dataset = train_dataset.map(pre_processing_dataset)
    print("Start Preprocessing validation dataset ...")
    validation_dataset = validation_dataset.map(pre_processing_dataset)

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(device)
    model.to(device)

    def compute_metrics(pred):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)

        f1 = f1_score(labels, preds, average="macro", labels=range(31)) * 100.0
        acc = accuracy_score(labels, preds)

        return {'macro f1': f1, 'accuracy': acc}

    training_args = TrainingArguments(
        output_dir='./results',          # output directory
        save_total_limit=5,              # number of total save model.
        save_steps=500,                 # model saving step.
        num_train_epochs=10,              # total number of training epochs
        learning_rate=1e-5,               # learning_rate
        per_device_train_batch_size=8,  # batch size per device during training
        per_device_eval_batch_size=8,   # batch size for evaluation
        warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler
        # weight_decay=0.01,               # strength of weight decay
        logging_dir='./logs',            # directory for storing logs
        logging_steps=100,              # log saving step.
        evaluation_strategy='steps', # evaluation strategy to adopt during training
                                    # `no`: No evaluation during training.
                                    # `steps`: Evaluate every `eval_steps`.
                                    # `epoch`: Evaluate every end of epoch.
        eval_steps = 500,            # evaluation step.
        load_best_model_at_end = True 
    )

    trainer = Trainer(
        model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,         # training dataset
        eval_dataset=validation_dataset,             # evaluation dataset
        compute_metrics=compute_metrics         # define metrics function
    )

    # train model
    print("Start Training ...")
    trainer.train()
    model.save_pretrained('./best_model')


    # dict0 = train_dataset[0:10]
    # print(dict0)
    # print()
    # input0 = f"""
    # ë¨¸ë¦¬ ê¸¸ì´ëŠ” {dict0['basestyle-type']}, ê³±ìŠ¬ê¸°ëŠ” {dict0['curl']}, ì•ë¨¸ë¦¬ëŠ” {dict0['bang']}, ì˜†ë¨¸ë¦¬ëŠ” {dict0['side']},
    # ê°€ë¥´ë§ˆëŠ” {dict0['partition']}, ì„±ë³„ì€ {dict0['sex']}, ëª¨ë°œ êµµê¸°ëŠ” {dict0['hair-width']}
    # """
    # print(input0)
    # print()

    # inputs = tokenizer(input0, return_tensors="pt")
    # print(inputs)

    # with torch.no_grad():
    #     logits = model(**inputs).logits

    # predicted_class_id = logits.argmax().item()
    # print(model.config.id2label[predicted_class_id])


    

if __name__ == "__main__":
    main()